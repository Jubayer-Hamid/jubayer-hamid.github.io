<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jubayer Ibn Hamid</title>
  
  <meta name="author" content="Jubayer Ibn Hamid">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jubayer Ibn Hamid</name>
              </p>
              <!-- <a href="https://www.stanford.edu">Stanford University studying Mathematical Physics. -->
              <p> I am a researcher at <a href="https://ai.stanford.edu">Stanford Artificial Intelligence Laboratory (SAIL)</a> where I am advised by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> in IRIS Lab. I am currently pursuing a B.S in Mathematical Physics and a coterminal M.S in Computer Science at Stanford University.
              <p style="text-align:center">
              <p>
                My research interests are in the intersection of machine learning, offline reinforcement learning, representation learning and foundation models. 
              <p style="text-align: center">  
              <p>
                Previously, I did research in physics. Outside of machine learning, I am quite interested in various fields of mathematics like abstract algebra, algebraic and differential topology/geometry.
              <p style="text-align: center">  
              <!-- <p>
                In my free time, I watch a lot of football - I am a diehard fan of FC Barcelona. I also watch a lot of F1 where I support Mercedes. Other than that, I like listening to music, reading and going on long walks.
              <p style="text-align: center">   -->

                <a href="mailto:jubayer@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/jubayer_hamid">Twitter</a> &nbsp/&nbsp
                <!-- <a href="https://github.com/Jubayer-Hamid">LinkedIn</a> &nbsp/&nbsp -->
                <a href="https://github.com/Jubayer-Hamid">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
              <a href="images/jubayer_circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jubayer_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                (*) denotes equal contribution
              </p>
            
              <!-- <p>
              I am broadly interested in researching scalable methods for helping robots acquire complex behaviours through learning. To that end, I am also interested in adjacent fields such as generative AI and optimisation. I am currently working on disentangled representation learning. 
              </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Tripod.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://kayburns.github.io/segmentingfeatures/"> -->
                <!-- <papertitle> RD3: Disentangling Representations in Quantized Latent Spaces>
              </a>
              <br>
              <a href="https://www.kylehsu.org/">Kyle Hsu*</a>, 
              <strong>Jubayer Ibn Hamid*</strong>,
              <a href="https://kayburns.github.io">Kaylee Burns</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>. 
              <br>
              <a href="https://arxiv.org/abs/2312.12444">Paper</a> /
              <a href="https://kayburns.github.io/segmentingfeatures/">Website</a>
              <p>
              <em></em>(Submitted to ICML 2024)</em>
              </p>
            </td>
          </tr>           -->

          <!-- ################### Tripod ###################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Tripod_fig1.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle> Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning</papertitle>
              </a>
              <br>
              <a href="https://www.kylehsu.org/">Kyle Hsu*</a>, 
              <strong>Jubayer Ibn Hamid*</strong>,
              <a href="https://kayburns.github.io">Kaylee Burns</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>. 
              <br>
              <p>
                Ongoing project. 
              </p>
              <p>
                We endow autoencoders with three inductive biases for disentanglement - latent quantization, latent multiinformation regularization (using kernel density approximation), and
                a scale-invariant, normalized Hessian (off-diagonal) penalty. Our resulting model achieves state-of-the-art results on disentanglement benchmarks. 
              </p>
            </td>
          </tr>	

          <!-- ############### Pre-trained Visual Representations ############### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pvrs.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://kayburns.github.io/segmentingfeatures/">
                <papertitle>What Makes Pre-trained Visual Representations Successful 
                  For Robust Manipulation?</papertitle>
              </a>
              <br>
              <a href="https://kayburns.github.io">Kaylee Burns</a>,
              <a href="">Zach Witzel</a>, 
              <strong>Jubayer Ibn Hamid</strong>,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>, 
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, 
              <a href="https://karolhausman.github.io/">Karol Hausman</a>
              <br>
              <a href="https://arxiv.org/abs/2312.12444">Paper</a> /
              <a href="https://kayburns.github.io/segmentingfeatures/">Website</a>
              <p>
                We find that visual representations designed for manipulation and control tasks do not necessarily generalize under subtle changes in lighting and scene texture or the introduction of distractor objects. To understand what properties do lead to robust representations, we compare the performance of 15 pre-trained vision models under different visual appearances. We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT models. The rank order induced by this metric is more predictive than metrics that have previously guided generalization research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by cue-conflict performance.
              </p>
            </td>
          </tr>		
				
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Notes</heading>
              <p>
                I am sharing notes on various topics that have fascinated me. These are not meant to be in-depth. Rather, they are meant to cover some of the basic constructions that show up periodically and are also interesting in and of themselves. 
              </p>
              <p>
                <a href="data/Real_and_Complex_Projective_Space.pdf">Real and Complex Projective Space</a>. These spaces that one can find in any introduction to topology/geometry is very fascinating. For example, one can show that any connected, compact surface is homeomorphic to either a genus or a connected sum of real projective spaces. I also think it is surprising to the human intuition that they are manifolds. &nbsp;&nbsp; 
              </p>
              <p>
                <a href="data/Policy_Gradient_Methods.pdf">Policy Gradient Methods</a>. These notes cover mostly the basic building blocks (including the policy gradient theorems for both episodic and continuing tasks) of the most basic policy gradient algorithms without any focus on implementation whatsoever. &nbsp;&nbsp; 
              </p>
            </td>
          </tr>
        </tbody></table>	




        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
